
## Vision-Language-Action Models (VLA)

**Arxiv**: [Vision-Language-Action Models: Concepts, Progress, Applications and Challenges](https://arxiv.org/html/2505.04769v1)<br>
<p><img width="50%" height="50%" src="https://arxiv.org/html/2505.04769v1/x1.png"></p>

![](https://arxiv.org/html/2505.04769v1/x3.png)
![](https://arxiv.org/html/2505.04769v1/x4.png)

### [OpenVLA](https://github.com/openvla/openvla)

#### [Open X-Embodiment](https://github.com/google-deepmind/open_x_embodiment)
**Arxiv**: [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/html/2310.08864v9)<br>

<p><img width="50%" height="50%" src="https://github.com/google-deepmind/open_x_embodiment/raw/main/imgs/teaser.png"></p>

#### [Robotics Transformer (RT-1)](https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/)
![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj11ho9tm4Td7ByTigAgDxFWsxbsZ6tQsAng3AtwuufHRuoLaLOV9YN7FUMyyAhefzuFOVCrbwTLsEaRYidOOToS__KRrotot-6aBxTliZxYz-B2jiJG-4myq5NB3vRKaY86nr5y1-13dBv_H_XyfnDijphCM3UBalczim0PeGJ63Z0Ok6k9zvKQ2D55A/s16000/image1.png)

